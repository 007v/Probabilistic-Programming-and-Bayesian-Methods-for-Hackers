{
 "metadata": {
  "name": "Chapter1_Introduction"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Chapter 1\n",
      "======"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Philosophy of Bayesian Inference\n",
      "------\n",
      "\n",
      "Think of what you would do in the following situation:\n",
      "\n",
      "> You are a pretty skilled programming, but like everyone, still miss a few bugs in your code. After a particularly difficult implementation of an algorithm, you decide to test your code on a trivial example. It passes. You test the code on a harder problem. It passes again! And it passes the next, *even more difficult*, test too. You are starting to believe that there are may be no bugs present...\n",
      "\n",
      "If you think this way, then congratulations, you're already a Bayesian practioner! Bayesian analysis is simplying updating your beliefs. A Bayesian can rarely be certain about a result, but he or she can be very confident. In the example above. we can never be 100% sure that our code is bug-free unless we test it on every possible problem, something rarely possible in practice! But we can test it on a large number of problems, it if it succeeds we can feel more confident about the code.  Bayesian analysis works identically: we can only update our beliefs about an outcome, rarely can we be absolutely sure unless we rule out all other alternatives. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Practice of Bayesian Inference\n",
      "------\n",
      "\n",
      "Bayesian inference differs from more traditional statistical anaylsis by preserving *uncertainity* about our beliefs. To really see how Bayesian inference is different, we need to compare it to an alternative inference method. Frequentist analysis is one such alternative. If frequentist and Bayesian analysis were computer programming functions, with inputs being statistical problems, then the two would differ in what they return to the user. The frequentist analysis function would return a single number, whereas the Bayesian function would return a distribution.\n",
      "\n",
      "For example, in our debugging problem above, calling the frequentist function with the argument \"My code passed each $X$ tests; is my code bug-free?\" would return a \"YES\". \\footnote1 On the other hand, asking our Bayesian function \"Often my code has bugs. My code passed all $X$ tests. Are there any bugs in my code?\" would return something very different: a distribution over \"YES\" and \"NO\". For example, it might return \n",
      "\n",
      "\"YES, with probability 0.8; NO, with probability 0.2\"\n",
      "\n",
      "Notice that the Bayesian function accepts an additional argument: \"Often my code has bugs\". This paramter, called the prior, is that computer in your head that says \"wait- something looks different with this situation\", or conversly \"yes, this is what I expected\". In our example, the programmer often sees debugging tests fail (and hence bugs are present), but this time we didn't, which signals an alert in our head. Techincally this parameter in the Bayesian function is optional, but we will see excluding it has its own consequences. \n",
      "\n",
      "At first, you might prefer the frequentist function. After all, it is confident: it is essentially says \"YES; with probability 1.\" This is great in cases where you need to provide a confident answer. For example, it is easier to communicate your boss \"YES\" than \"YES; with 80% probability\". Also, you do not need to provide a prior argument, thus making things simpler when you can't think of a good prior. But we will see that choosing the frequentist analysis is too hasty.\n",
      "\n",
      "\\footnote1{ This is because we can model the problem in a frequentist setting by thinking of each test as a bernoulli random variable, thus all tests are a Binomial variable. If all test pass, then the MLE (of a Binomial) is $X/X = 1$, meaning the MLE result suggests no bugs are present}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Another example:\n",
      "\n",
      ">  You have aquired a massive amount of text message data from users of your system.   \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probability and statistics\n",
      "------\n",
      "\n",
      "**Let's just quickly recall what a distribution is:** Let $Z$ be some random variable. Then associated with $Z$ is a distribution function that assigns probabilites to the different values $Z$ can take. There are two cases:\n",
      "-  **$Z$ is discrete**: Discrete random variables cannot be just any value. Things like populations, movie ratings, and number of votes are all discrete random variables. It's more clear when we contrast it with...\n",
      "-  **$Z$ is continuous**: Continuous random variable can take on arbitrarily precise values. For example, tempature, speed, time, color are all modeled as continous variables. \n",
      "\n",
      "###Discrete Case\n",
      "If $Z$ is discrete, then is distribution is called a *probability mass function*, which measures the probability $Z$ takes on the value $k$, denoted $P(Z=k)$. Note that the probability mass function completely describes the random variable $Z$, that is, if we know the mass function, we know how $Z$ should behave. There are some popular probability mass functions that you should be aware of: we will be using them in the remainder of the book. We will introduce them as needed, but let's introduce the first very useful probability mass function. We say $Z$ is *Poisson*-distributed if:\n",
      "\n",
      "$$P(Z = k) =\\frac{ \\lambda^k e^{-\\lambda} }{k!} \\; \\; k=0,1,2, \\dots $$\n",
      "\n",
      "What is $\\lambda$? It is called a parameter, and it describes the shape of the distribution. By increasing $\\lambda$, we add more probability to larger values, and conversly by decreasing $\\lambda$ we add more probability to smaller values. Notice that $k$ **must** be a non-negative integer, i.e., $k$ must take on values 0,1,2, and so on. This is very important, because if you wanted to model a population you would not be interested in populations with 4.25 or 5.612401 members.\n",
      "\n",
      "###Continuous Case\n",
      "Instead of a probability mass function, a continous random variable has a probability denisty function. This might seem like unnecessary nomenclature, but the denisty function and the mass function are very different. Whereas mass functions have a probablistic interpretation, density functions do not. For example, commonly a continuous random variable has a *exponential density*. The density function for an exponential random variable looks like:\n",
      "\n",
      "$$f_Z(x) = \\frac{1}{\\lambda} e^{-\\lambda x }$$\n",
      "\n",
      "Like the Poisson random variable, an exponential random variable can only take on non-negative values. But unlike a random variable, the exponential can take on *any* non-negative values, like 4.25 or 5.612401. This makes is a poor choice for count data, but a great choice for time data, or tempature data (measured in Kelvins, of course). \n",
      "\n",
      "###But what is $\\lambda$?!\n",
      "\n",
      "Well, that question is what motivates statistics. In the real world, $\\lambda$ is hidden from us. We only see $Z$, and must go backwards to try and determine $\\lambda$. The problem is so difficult because there is not a one-to-one mapping from $Z$ to $\\lambda$. Many different methods --no, many  different *philosophies* have been created to solve the problem of estimating $\\lambda$. But since $\\lambda$ is never actually observed, no one can say for certain which philosophy is correct (most often)!\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The central idea behind Bayesian inference\n",
      "-----\n",
      "The Bayesian method believes that probability is better seen as a measure of knowledge. This is the natural interpretation of probability. Again, for this to be clearer, we consider the alternative: Frequentist methods assume that probability is the long-run frequency of events (hence the name). For example, the probability of plane accidents under a frequentist method is the long-term frequency of plane accidents. This makes sense for many probabilities, but breaks down when events do not occur repeatedly. Consider: we often assign probabilities to a presidential election, but the election itself only happens once! Frequentist get around this by invoking alternate realities [cite], but Bayesians have a more intuitive approach. Bayesians beleive that a probability is a measure of a belief in an event occuring. This definition agrees with the plane accident example, for having observed the frequency of plane accidents, your belief should be exactly that frequency. Similarly, under this definition of probability, we can speak about a probability (read: belief) of a presidential election outcome. \n",
      "\n",
      "Think about how we can extend this definition of probability to things that are not *really* random, but to anything that we are unsure about. \n",
      "-  That code you wrote has a bug in it or does not have a bug in it, but we do not know the answer. Though we have a belief about the presence or absence of a bug.  \n",
      "-  A medical patient is exhibiting so and so symptoms. There are a number of diseases that could be causing all of them,  but only has a single disease is present. A doctor has beliefs about which one.\n",
      "- My income next year will only *occur* once, and subject to some factors my income can change. I have a belief about what it might be. \n",
      "\n",
      "This idea of treating beliefs as probability is natual to humans, and in fact the frequentist version of probability is what is learned. \n",
      "\n",
      "John Maynard Keynes, the great economist and thinker, said \n",
      "> \"When the facts change, I change my mind; Do you?\" [cite]\n",
      "\n",
      "\n",
      "To extend our notation of distributions to include the paramters, we denote $P( Z | \\theta)$ to be the probability of observing $Z$ with parameters $\\theta$. Now, "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Introduction to our first hammer: PyMC\n",
      "-------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why would I want to sample from the posterior, anyways?\n",
      "-------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___________________"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Technical Readers\n",
      "========"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You've already been doing Bayesian analysis!\n",
      "------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}