{
 "metadata": {
  "name": "LossFunctions"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Chapter 4: Would you rather lose an arm or a leg?\n",
      "____"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Statisticians can be a sour bunch. Instead of considering thier winnings, the only worry about how much they are losing. In fact, they consider their wins to be negative loses! \n",
      "\n",
      "The author Nassim Taleb of *The Black Swan* and *Antifragility* stresses the importance of *payoffs* of decisions, *not the accuracy*. For example, consider the following vignette:\n",
      "\n",
      ">   A meteorologist is predicting the probability of a possible hurricane striking his region. He estimates, with 95% confidence (whatever this means), that the probability of it *not* striking is between 99% - 100%. He is very happy with his precision and advises the city against major evacuation. Unfortunately, the hurricane does strike and the city is crushed. \n",
      "\n",
      "The stylized example shows the flaw in using a pure accuracy metrix to measre how good your estimate is, while an appealing and *objective* thing to do, misses the point: decision making.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loss Functions\n",
      "\n",
      "We introduce what statisticians and decisions theorist call *loss functions*. A loss function is a function of the true parameter, and an estimate of that parameter\n",
      "\n",
      "$$ L( \\theta, \\hat{theta} ) = f( \\theta, \\hat{theta} )$$\n",
      "\n",
      "The important point of loss functions is that it measures how *bad* our current estimate is: the large the loss, the worse the estimate. For example, the *squared-error loss* is a very common loss function, defined:\n",
      "\n",
      "$$ L( \\theta, \\hat{theta} ) = ( \\theta -  \\hat{theta} )^2$$\n",
      "\n",
      "The squared-error loss function is used in estimators like linear regression and many areas of machine learning \\cite. We can also consider an asymmetric loss function, something like:\n",
      "\n",
      "$$ L( \\theta, \\hat{theta} ) = peicewise ( \\theta -  \\hat{theta} )^2$$\n",
      "\n",
      "which means estimating smaller than the true estimate is better than above. A situation where this might be useful is in estimating next month's sales, where a more pessemistic outlook is preferred to avoid overallocation of resources. A negative thing about the squared-error loss is that is puts more emphasis on large outliers as the loss increases quadraticly. The mroe *robust* loss function that increases linearly is the *absolute-loss*\n",
      "\n",
      "$$ L( \\theta, \\hat{theta} ) = | \\theta -  \\hat{theta} | $$\n",
      "\n",
      "Other popular loss function include:\n",
      "-  $$ L( \\theta, \\hat{theta} ) = \\mathbb{1}_{ \\hat{\\theta} \\neq \\theta } $$ is the zero-one loss used often in machine learning classification algorithms.\n",
      "-  $$ L( \\theta, \\hat{theta} ) = \\hat{theta}*\\log( \\theta ) + (1-\\hat{ \\theta})\\log( 1 - theta ), \\; \\; \\hat{theta}, theta \\in [0,1]$$, called the *log-loss*, also used in machine learning.\n",
      "-  $$ Huber loss$$ is a mix of square and absolute loss functions.\n",
      "\n",
      "Histroically, loss functions have been motivated more from 1) mathematical convience, and 2) they are robust to application, i.e., the are objective measures of loss. The first reason has really held back the full breath of loss functions in application. With computers being agnogstic to mathematical convience, we are free to design our own loss functions, for example:\n",
      "\n",
      "- $$ L( \\theta, \\hat{theta} ) = \\frac{ | \\theta - \\hat{theta} | }{ \\theta(1-theta) }, \\; \\; \\hat{theta}, theta \\in [0,1] $$ emphasizes an estimate closer to 0 or 1 since if the true value $\\theta$ is near 0 or 1, the loss will be *very* large unless $\\hat{\\theta}$ is equally as close 0 or 1. \n",
      "-  $$L( \\theta, \\hat{theta} ) =  1 - \\exp \\left( (\\theta -  \\hat{theta} )^2 \\right) $$ is bounded between 0 and 1.\n",
      "-  $$ L( \\theta, \\hat{theta} ) = \\frac{\\hat{theta}}{\\theta}, \\;\\; \\theta, \\hat{\\theta} > 0 $$ is a ratio loss"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}